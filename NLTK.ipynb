{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preposseing steps\n",
    "# 1. convert all data to lower case\n",
    "# 2. remove numbers\n",
    "# 3. remove punctuation\n",
    "# 4. remove default stopwords\n",
    "# 5. stemming\n",
    "# 6. lemmatization\n",
    "# 7. part of speech (pos) tagging\n",
    "# 8. chunking\n",
    "# 9. Named Entity Recognition\n",
    "# 10. Text normalization\n",
    "# 11. Word Count\n",
    "# 12. Frequency distribution\n",
    "# 13. n-grams\n",
    "# 14. Zipf's Law\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weather is too cloudy. possiblity of rain is high, today!!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. convert all data to lower case\n",
    "# We do lowercase the text to reduce the size of the vocabulary of our text data.\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "s = \"Weather is too Cloudy. Possiblity of Rain is High, Today!!\"\n",
    "lowercase_text(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You bought  candies from shop, and  candies are in home.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. remove numbers\n",
    "# We should either remove the numbers or convert those numbers into textual representations.\n",
    "# We use regular expressions(re) to remove the numbers.\n",
    "def remove_num(text):\n",
    "    res = re.sub(r\"\\d+\", \"\", text)\n",
    "    return res\n",
    "\n",
    "s = \"You bought 6 candies from shop, and 4 candies are in home.\"\n",
    "remove_num(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You bought six candies from shop, and four candies are in home.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. remove numbers\n",
    "# We use inflect library to convert the numbers into words.\n",
    "import inflect\n",
    "inflector = inflect.engine()\n",
    "\n",
    "def convert_num(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word.isdigit():\n",
    "            new_text.append(inflector.number_to_words(word))\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    \n",
    "    return \" \".join(new_text)\n",
    "\n",
    "s = \"You bought 6 candies from shop, and 4 candies are in home.\"\n",
    "convert_num(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey Are you excited After a week we will be in Shimla'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. remove punctuation\n",
    "# We remove punctuations because of that we don't have different form of the same word. \n",
    "# If we don't remove punctuations, then been, been, and been! will be treated separately.\n",
    "def remove_punct(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "s = \"Hey, Are you excited??, After a week, we will be in Shimla!!!\"\n",
    "remove_punct(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/calkikhunt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/calkikhunt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Data', 'new', 'oil', '.', 'A.I', 'last', 'invention', '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. remove default stopwords\n",
    "# Stopwords are words that do not contribute to the meaning of the sentence. \n",
    "# Hence, they can be safely removed without causing any change in the meaning of a sentence. \n",
    "# The NLTK(Natural Language Toolkit) library has the set of stopwords and we can use these \n",
    "# to remove stopwords from our text and return a list of word tokens.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "s = \"Data is the new oil. A.I is the last invention.\"\n",
    "remove_stopwords(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'is',\n",
       " 'the',\n",
       " 'new',\n",
       " 'revolut',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'day',\n",
       " 'one',\n",
       " 'individu',\n",
       " 'would',\n",
       " 'gener',\n",
       " 'terabyt',\n",
       " 'of',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. stemming\n",
    "# From Stemming we will process of getting the root form of a word. \n",
    "# Root or Stem is the part to which inflextional affixes(like -ed, -ize, etc) are added. \n",
    "# We would create the stem words by removing the prefix of suffix of a word. So, stemming a word may not result in actual words.\n",
    "\n",
    "# For Example: Mangoes ---> Mango\n",
    "#             Boys ---> Boy\n",
    "#             going ---> go\n",
    "             \n",
    "# If our sentences are not in tokens, then we need to convert it into tokens. \n",
    "# After we converted strings of text into tokens, then we can convert those word tokens into their root form. \n",
    "# These are the Porter stemmer, the snowball stemmer, and the Lancaster Stemmer. We usually use Porter stemmer among them.\n",
    "\n",
    "# The process of reducing a word to its stem or root format.\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "portal_stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stems = [portal_stemmer.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "s = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
    "stem_words(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/calkikhunt/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Data',\n",
       " 'be',\n",
       " 'the',\n",
       " 'new',\n",
       " 'revolution',\n",
       " 'in',\n",
       " 'the',\n",
       " 'World',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'day',\n",
       " 'one',\n",
       " 'individual',\n",
       " 'would',\n",
       " 'generate',\n",
       " 'terabytes',\n",
       " 'of',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. lemmatization\n",
    "# As stemming, lemmatization do the same but the only difference is that lemmatization ensures that \n",
    "# root word belongs to the language. Because of the use of lemmatization we will get the valid words. \n",
    "# In NLTK(Natural language Toolkit), we use WordLemmatizer to get the lemmas of words. \n",
    "# We also need to provide a context for the lemmatization.So, we added pos(parts-of-speech) as a parameter.\n",
    "\n",
    "# The transformation that uses a dictionary to map a wordâ€™s variant back to its root format\n",
    "\n",
    "from nltk.stem import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemma = wordnet.WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    word_token = word_tokenize(text)\n",
    "    lemmas = [lemma.lemmatize(word, pos=\"v\") for word in word_token]\n",
    "    return lemmas\n",
    "\n",
    "s = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
    "lemmatize_word(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/calkikhunt/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Are', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('afraid', 'IN'),\n",
       " ('of', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. part of speech (pos) tagging\n",
    "# The pos(parts of speech) explain you how a word is used in a sentence. In the sentence, \n",
    "# a word have different contexts and semantic meanings. The basic natural language processing(NLP) models \n",
    "# like bag-of-words(bow) fails to identify these relation between the words. For that we use pos tagging \n",
    "# to mark a word to its pos tag based on its context in the data. Pos is also used to extract relationship between the words.\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "def pos_tagg(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    return pos_tag(word_tokens)\n",
    "\n",
    "pos_tagg(\"Are you afraid of something?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ red/JJ parrot/NN)\n",
      "  is/VBZ\n",
      "  flying/VBG\n",
      "  in/IN\n",
      "  (NP the/DT sky/NN))\n",
      "(NP the/DT little/JJ red/JJ parrot/NN)\n",
      "(NP the/DT sky/NN)\n"
     ]
    }
   ],
   "source": [
    "# 8. chunking\n",
    "# Chunking is the process of extracting phrases from the Unstructured text and give them more structure to it. \n",
    "# We also called them shallow parsing. We can do it on top of pos tagging. It groups words into chunks mainly \n",
    "# for noun phrases. chunking we do by using regular expression. \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "def chunking(text, grammer):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    word_pos = pos_tag(word_tokens)\n",
    "    chunkParser = nltk.RegexpParser(grammer)\n",
    "    tree = chunkParser.parse(word_pos)\n",
    "    for subtree in tree.subtrees():\n",
    "        print(subtree)\n",
    "    tree.draw()\n",
    "\n",
    "s = \"the little red parrot is flying in the sky\"\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunking(s, grammar)\n",
    "\n",
    "# In the above example, we defined the grammar by using the regular expression rule. \n",
    "# This rule tells you that NP(noun phrase) chunk should be formed whenever the chunker find the optional determiner(DJ) \n",
    "# followed by any no. of adjectives and then a NN(noun).\n",
    "# Libraries like Spacy and TextBlob are best for chunking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/calkikhunt/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/calkikhunt/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,1032.0,168.0\" width=\"1032px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"6.20155%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Brain</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"3.10078%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.20155%\" x=\"6.20155%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Lara</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"9.30233%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.20155%\" x=\"12.4031%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">scored</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.5039%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.87597%\" x=\"18.6047%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20.5426%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.97674%\" x=\"22.4806%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">highest</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25.969%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.87597%\" x=\"29.4574%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">400</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.3953%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.65116%\" x=\"33.3333%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">runs</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"35.6589%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.10078%\" x=\"37.9845%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"39.5349%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.10078%\" x=\"41.0853%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"42.6357%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.65116%\" x=\"44.186%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">test</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"46.5116%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.42636%\" x=\"48.8372%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">match</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"51.5504%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.42636%\" x=\"54.2636%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">which</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">WDT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"56.9767%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.20155%\" x=\"59.6899%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">played</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.7907%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.10078%\" x=\"65.8915%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"67.4419%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.97674%\" x=\"68.9922%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">between</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.4806%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"10.8527%\" x=\"75.969%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">ORGANIZATION</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">WI</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"81.3953%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.87597%\" x=\"86.8217%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"88.7597%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.97674%\" x=\"90.6977%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">England</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"94.186%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.32558%\" x=\"97.6744%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"98.8372%\" y1=\"1.2em\" y2=\"3em\" /></svg>",
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('Brain', 'NNP')]), Tree('PERSON', [('Lara', 'NNP')]), ('scored', 'VBD'), ('the', 'DT'), ('highest', 'JJS'), ('400', 'CD'), ('runs', 'NNS'), ('in', 'IN'), ('a', 'DT'), ('test', 'NN'), ('match', 'NN'), ('which', 'WDT'), ('played', 'VBD'), ('in', 'IN'), ('between', 'IN'), Tree('ORGANIZATION', [('WI', 'NNP')]), ('and', 'CC'), Tree('GPE', [('England', 'NNP')]), ('.', '.')])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9. Named Entity Recognition\n",
    "# It is used to extract information from unstructured text. It is used to classy the entities which is \n",
    "# present in the text into categories like a person, organization, event, places, etc. This will give you \n",
    "# a detail knowledge about the text and the relationship between the different entities.\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download(\"words\")\n",
    "\n",
    "def ner(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    word_pos = pos_tag(word_tokens)\n",
    "    return ne_chunk(word_pos)\n",
    "\n",
    "s = \"Brain Lara scored the highest 400 runs in a test match which played in between WI and England.\"\n",
    "ner(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Text normalization\n",
    "# In the text pre-processing highly overlooked step is text normalization. The text normalization means \n",
    "# the process of transforming the text into the canonical(or standard) form. Like, \"ok\" and \"k\" can be \n",
    "# transformed to \"okay\", its canonical form. And another example is mapping of near identical words such \n",
    "# as \"preprocessing\", \"pre-processing\" and \"pre processing\" to just \"preprocessing\".\n",
    "\n",
    "# Text normaliztion is too useful for noisy text such as social media comments, comment to blog posts, \n",
    "# text messages, where abbreviations, misspellings, and the use out-of-vocabulary(oov) are prevalent.\n",
    "\n",
    "# Text normalization has even been effective for analyzing highly unstructured clinical texts where physicians \n",
    "# take notes in non-standard ways. We have also found it useful for topic extraction where near synonyms and \n",
    "# spelling differences are common (like 'topic modelling', 'topic modeling', 'topic-modeling', 'topic-modelling').\n",
    "\n",
    "# Unlike stemming and lemmatization, there is not a standard way to normalize texts. It typically depends on \n",
    "# the task. For e.g, the way you would normalize clinical texts would arguably be different from how you normalize text messages.\n",
    "\n",
    "# Some of the common approaches to text normalization include dictionary mappings, statistical machine \n",
    "# translation (SMT) and spelling-correction based approaches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Word Count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'There\",\n",
       " 'is',\n",
       " 'no',\n",
       " 'need',\n",
       " 'to',\n",
       " 'panic.',\n",
       " 'We',\n",
       " 'need',\n",
       " 'to',\n",
       " 'work',\n",
       " 'together,',\n",
       " 'take',\n",
       " 'small',\n",
       " 'yet',\n",
       " 'important',\n",
       " 'measures',\n",
       " 'to',\n",
       " 'ensure',\n",
       " \"self-protection,'\",\n",
       " 'the',\n",
       " 'Prime',\n",
       " 'Minister',\n",
       " 'tweeted.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.regexp import WhitespaceTokenizer\n",
    "s = \"'There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,' the Prime Minister tweeted.\"\n",
    "WhitespaceTokenizer().tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\",\n",
       " 'There',\n",
       " 'is',\n",
       " 'no',\n",
       " 'need',\n",
       " 'to',\n",
       " 'panic',\n",
       " '.',\n",
       " 'We',\n",
       " 'need',\n",
       " 'to',\n",
       " 'work',\n",
       " 'together',\n",
       " ',',\n",
       " 'take',\n",
       " 'small',\n",
       " 'yet',\n",
       " 'important',\n",
       " 'measures',\n",
       " 'to',\n",
       " 'ensure',\n",
       " 'self',\n",
       " '-',\n",
       " 'protection',\n",
       " \",'\",\n",
       " 'the',\n",
       " 'Prime',\n",
       " 'Minister',\n",
       " 'tweeted',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.regexp import WordPunctTokenizer\n",
    "s = \"'There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,' the Prime Minister tweeted.\"\n",
    "WordPunctTokenizer().tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 23 samples and 28 outcomes>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Samples', ylabel='Counts'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 12. Frequency distribution\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = \"'There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,' the Prime Minister tweeted.\"\n",
    "freqDist = FreqDist(word_tokenize(s))\n",
    "print(freqDist)\n",
    "freqDist.plot(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Now', 'is', 'better', 'than'),\n",
       " ('is', 'better', 'than', 'never'),\n",
       " ('better', 'than', 'never', '.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 13. n-grams\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = \"Now is better than never.\"\n",
    "list(ngrams(word_tokenize(s), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Zipf's Law\n",
    "\n",
    "Zipfâ€™s Law is a statistical distribution in certain data sets, such as words in a linguistic corpus, in which the frequencies of certain words are inversely proportional to their ranks. Named for linguist George Kingsley Zipf, who around 1935 was the first to draw attention to this phenomenon, the law examines the frequency of words in natural language and how the most common word occurs twice as often as the second most frequent word, three times as often as the subsequent word and so on until the least frequent word. The word in the position n appears 1/n times as often as the most frequent one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/cache/epub/345/pg345.txt\"\n",
    "content = requests.get(url)\n",
    "text_string = content.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'Dracula',\n",
       " 'Bram',\n",
       " 'Stoker',\n",
       " 'This',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'the',\n",
       " 'United',\n",
       " 'States',\n",
       " 'and',\n",
       " 'most',\n",
       " 'other',\n",
       " 'parts',\n",
       " 'the',\n",
       " 'world',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'almost',\n",
       " 'whatsoever',\n",
       " 'You',\n",
       " 'may',\n",
       " 'copy',\n",
       " 'give',\n",
       " 'away',\n",
       " 'use',\n",
       " 'under',\n",
       " 'the',\n",
       " 'terms',\n",
       " 'the',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'License',\n",
       " 'included',\n",
       " 'with',\n",
       " 'this',\n",
       " 'online',\n",
       " 'www',\n",
       " 'gutenberg',\n",
       " 'org',\n",
       " 'you',\n",
       " 'are',\n",
       " 'not',\n",
       " 'located',\n",
       " 'the',\n",
       " 'United',\n",
       " 'States',\n",
       " 'you',\n",
       " 'will',\n",
       " 'have',\n",
       " 'check',\n",
       " 'the',\n",
       " 'laws',\n",
       " 'the',\n",
       " 'country',\n",
       " 'where',\n",
       " 'you',\n",
       " 'are',\n",
       " 'located',\n",
       " 'before',\n",
       " 'using',\n",
       " 'this',\n",
       " 'Title',\n",
       " 'Dracula',\n",
       " 'Author',\n",
       " 'Bram',\n",
       " 'Stoker',\n",
       " 'Release',\n",
       " 'Date',\n",
       " 'October',\n",
       " 'Most',\n",
       " 'recently',\n",
       " 'updated',\n",
       " 'July',\n",
       " 'Language',\n",
       " 'English',\n",
       " 'Produced',\n",
       " 'Chuck',\n",
       " 'Greif',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Online',\n",
       " 'Team',\n",
       " 'Bram',\n",
       " 'Stoker',\n",
       " 'colophon',\n",
       " 'Copyright',\n",
       " 'the',\n",
       " 'United',\n",
       " 'States',\n",
       " 'America',\n",
       " 'according',\n",
       " 'Act',\n",
       " 'Congress',\n",
       " 'Bram',\n",
       " 'Stoker',\n",
       " 'rights',\n",
       " 'reserved',\n",
       " 'Contents',\n",
       " 'Jonathan',\n",
       " 'Harker',\n",
       " 'Journal',\n",
       " 'Jonathan',\n",
       " 'Harker',\n",
       " 'Journal',\n",
       " 'Jonathan',\n",
       " 'Harker',\n",
       " 'Journal',\n",
       " 'Jonathan',\n",
       " 'Harker',\n",
       " 'Journal',\n",
       " 'Letters',\n",
       " 'Lucy',\n",
       " 'and',\n",
       " 'Mina',\n",
       " 'Mina',\n",
       " 'Murray',\n",
       " 'Journal',\n",
       " 'Cutting',\n",
       " 'from',\n",
       " 'The',\n",
       " 'Dailygraph',\n",
       " 'August',\n",
       " 'Mina',\n",
       " 'Murray',\n",
       " 'Journal',\n",
       " 'Mina',\n",
       " 'Murray',\n",
       " 'Journal',\n",
       " 'Mina',\n",
       " 'Murray',\n",
       " 'Journal',\n",
       " 'Lucy',\n",
       " 'Westenra',\n",
       " 'Diary',\n",
       " 'Seward',\n",
       " 'Diary',\n",
       " 'Seward',\n",
       " 'Diary',\n",
       " 'Mina',\n",
       " 'Harker',\n",
       " 'Journal',\n",
       " 'Seward',\n",
       " 'Diary',\n",
       " 'Seward',\n",
       " 'Diary',\n",
       " 'Seward',\n",
       " 'Diary',\n",
       " 'Seward',\n",
       " 'Diary',\n",
       " 'Jonathan',\n",
       " 'Harker',\n",
       " 'Journal',\n",
       " 'Jonathan',\n",
       " 'Harker',\n",
       " 'Journal',\n",
       " 'Seward',\n",
       " 'Diary',\n",
       " 'Jonathan',\n",
       " 'Harker',\n",
       " 'Journal',\n",
       " 'Seward',\n",
       " 'Diary',\n",
       " 'Seward',\n",
       " 'Phonograph',\n",
       " 'Diary',\n",
       " 'spoken',\n",
       " 'Van',\n",
       " 'Helsing',\n",
       " 'Seward',\n",
       " 'Diary',\n",
       " 'Seward',\n",
       " 'Diary',\n",
       " 'Mina',\n",
       " 'Harker',\n",
       " 'Journal',\n",
       " 'shorthand',\n",
       " 'May',\n",
       " 'Bistritz',\n",
       " 'Left',\n",
       " 'Munich',\n",
       " 'May',\n",
       " 'arriving',\n",
       " 'Vienna',\n",
       " 'early',\n",
       " 'next',\n",
       " 'morning',\n",
       " 'should',\n",
       " 'have',\n",
       " 'arrived',\n",
       " 'but',\n",
       " 'train',\n",
       " 'was',\n",
       " 'hour',\n",
       " 'late',\n",
       " 'Buda',\n",
       " 'Pesth',\n",
       " 'seems',\n",
       " 'wonderful',\n",
       " 'place',\n",
       " 'from',\n",
       " 'the',\n",
       " 'glimpse',\n",
       " 'which',\n",
       " 'got',\n",
       " 'from',\n",
       " 'the',\n",
       " 'train',\n",
       " 'and',\n",
       " 'the',\n",
       " 'little',\n",
       " 'could',\n",
       " 'walk',\n",
       " 'through',\n",
       " 'the',\n",
       " 'streets',\n",
       " 'feared',\n",
       " 'very',\n",
       " 'far',\n",
       " 'from',\n",
       " 'the',\n",
       " 'station',\n",
       " 'had',\n",
       " 'arrived',\n",
       " 'late',\n",
       " 'and',\n",
       " 'would',\n",
       " 'start',\n",
       " 'near',\n",
       " 'the',\n",
       " 'correct',\n",
       " 'time',\n",
       " 'possible',\n",
       " 'The',\n",
       " 'impression',\n",
       " 'had',\n",
       " 'was',\n",
       " 'that',\n",
       " 'were',\n",
       " 'leaving',\n",
       " 'the',\n",
       " 'West',\n",
       " 'and',\n",
       " 'entering',\n",
       " 'the',\n",
       " 'East',\n",
       " 'the',\n",
       " 'most',\n",
       " 'western',\n",
       " 'splendid',\n",
       " 'bridges',\n",
       " 'over',\n",
       " 'the',\n",
       " 'Danube',\n",
       " 'which',\n",
       " 'here',\n",
       " 'noble',\n",
       " 'width',\n",
       " 'and',\n",
       " 'depth',\n",
       " 'took',\n",
       " 'among',\n",
       " 'the',\n",
       " 'traditions',\n",
       " 'Turkish',\n",
       " 'rule',\n",
       " 'left',\n",
       " 'pretty',\n",
       " 'good',\n",
       " 'time',\n",
       " 'and',\n",
       " 'came',\n",
       " 'after',\n",
       " 'nightfall',\n",
       " 'Here',\n",
       " 'stopped',\n",
       " 'for',\n",
       " 'the',\n",
       " 'night',\n",
       " 'the',\n",
       " 'Hotel',\n",
       " 'Royale',\n",
       " 'had',\n",
       " 'for',\n",
       " 'dinner',\n",
       " 'rather',\n",
       " 'supper',\n",
       " 'chicken',\n",
       " 'done',\n",
       " 'some',\n",
       " 'way',\n",
       " 'with',\n",
       " 'red',\n",
       " 'pepper',\n",
       " 'which',\n",
       " 'was',\n",
       " 'very',\n",
       " 'good',\n",
       " 'but',\n",
       " 'thirsty',\n",
       " 'get',\n",
       " 'recipe',\n",
       " 'for',\n",
       " 'Mina',\n",
       " 'asked',\n",
       " 'the',\n",
       " 'waiter',\n",
       " 'and',\n",
       " 'said',\n",
       " 'was',\n",
       " 'called',\n",
       " 'paprika',\n",
       " 'hendl',\n",
       " 'and',\n",
       " 'that',\n",
       " 'was',\n",
       " 'national',\n",
       " 'dish',\n",
       " 'should',\n",
       " 'able',\n",
       " 'get',\n",
       " 'anywhere',\n",
       " 'along',\n",
       " 'the',\n",
       " 'found',\n",
       " 'smattering',\n",
       " 'German',\n",
       " 'very',\n",
       " 'useful',\n",
       " 'here',\n",
       " 'indeed',\n",
       " 'don',\n",
       " 'know',\n",
       " 'how',\n",
       " 'should',\n",
       " 'able',\n",
       " 'get',\n",
       " 'without',\n",
       " 'Having',\n",
       " 'had',\n",
       " 'some',\n",
       " 'time',\n",
       " 'disposal',\n",
       " 'when',\n",
       " 'London',\n",
       " 'had',\n",
       " 'visited',\n",
       " 'the',\n",
       " 'British',\n",
       " 'Museum',\n",
       " 'and',\n",
       " 'made',\n",
       " 'search',\n",
       " 'among',\n",
       " 'the',\n",
       " 'books',\n",
       " 'and',\n",
       " 'maps',\n",
       " 'the',\n",
       " 'library',\n",
       " 'regarding',\n",
       " 'had',\n",
       " 'struck',\n",
       " 'that',\n",
       " 'some',\n",
       " 'the',\n",
       " 'country',\n",
       " 'could',\n",
       " 'hardly',\n",
       " 'fail',\n",
       " 'have',\n",
       " 'some',\n",
       " 'importance',\n",
       " 'dealing',\n",
       " 'with',\n",
       " 'nobleman',\n",
       " 'that',\n",
       " 'country',\n",
       " 'find',\n",
       " 'that',\n",
       " 'the',\n",
       " 'district',\n",
       " 'named',\n",
       " 'the',\n",
       " 'extreme',\n",
       " 'east',\n",
       " 'the',\n",
       " 'country',\n",
       " 'just',\n",
       " 'the',\n",
       " 'borders',\n",
       " 'three',\n",
       " 'states',\n",
       " 'Moldavia',\n",
       " 'and',\n",
       " 'Bukovina',\n",
       " 'the',\n",
       " 'midst',\n",
       " 'the',\n",
       " 'Carpathian',\n",
       " 'mountains',\n",
       " 'one',\n",
       " 'the',\n",
       " 'wildest',\n",
       " 'and',\n",
       " 'least',\n",
       " 'known',\n",
       " 'portions',\n",
       " 'Europe',\n",
       " 'was',\n",
       " 'not',\n",
       " 'able',\n",
       " 'light',\n",
       " 'any',\n",
       " 'map',\n",
       " 'work',\n",
       " 'giving',\n",
       " 'the',\n",
       " 'exact',\n",
       " 'locality',\n",
       " 'the',\n",
       " 'Castle',\n",
       " 'Dracula',\n",
       " 'there',\n",
       " 'are',\n",
       " 'maps',\n",
       " 'this',\n",
       " 'country',\n",
       " 'yet',\n",
       " 'compare',\n",
       " 'with',\n",
       " 'our',\n",
       " 'own',\n",
       " 'Ordnance',\n",
       " 'Survey',\n",
       " 'maps',\n",
       " 'but',\n",
       " 'found',\n",
       " 'that',\n",
       " 'Bistritz',\n",
       " 'the',\n",
       " 'post',\n",
       " 'town',\n",
       " 'named',\n",
       " 'Count',\n",
       " 'Dracula',\n",
       " 'fairly',\n",
       " 'well',\n",
       " 'known',\n",
       " 'place',\n",
       " 'shall',\n",
       " 'enter',\n",
       " 'here',\n",
       " 'some',\n",
       " 'notes',\n",
       " 'they',\n",
       " 'may',\n",
       " 'refresh',\n",
       " 'memory',\n",
       " 'when',\n",
       " 'talk',\n",
       " 'over',\n",
       " 'travels',\n",
       " 'with',\n",
       " 'Mina',\n",
       " 'the',\n",
       " 'population',\n",
       " 'there',\n",
       " 'are',\n",
       " 'four',\n",
       " 'distinct',\n",
       " 'Saxons',\n",
       " 'the',\n",
       " 'South',\n",
       " 'and',\n",
       " 'mixed',\n",
       " 'with',\n",
       " 'them',\n",
       " 'the',\n",
       " 'Wallachs',\n",
       " 'who',\n",
       " 'are',\n",
       " 'the',\n",
       " 'the',\n",
       " 'Dacians',\n",
       " 'Magyars',\n",
       " 'the',\n",
       " 'West',\n",
       " 'and',\n",
       " 'Szekelys',\n",
       " 'the',\n",
       " 'East',\n",
       " 'and',\n",
       " 'North',\n",
       " 'going',\n",
       " 'among',\n",
       " 'the',\n",
       " 'latter',\n",
       " 'who',\n",
       " 'claim',\n",
       " 'descended',\n",
       " 'from',\n",
       " 'Attila',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Huns',\n",
       " 'This',\n",
       " 'may',\n",
       " 'for',\n",
       " 'when',\n",
       " 'the',\n",
       " 'Magyars',\n",
       " 'conquered',\n",
       " 'the',\n",
       " 'country',\n",
       " 'the',\n",
       " 'eleventh',\n",
       " 'century',\n",
       " 'they',\n",
       " 'found',\n",
       " 'the',\n",
       " 'Huns',\n",
       " 'settled',\n",
       " 'read',\n",
       " 'that',\n",
       " 'every',\n",
       " 'known',\n",
       " 'the',\n",
       " 'world',\n",
       " 'gathered',\n",
       " 'into',\n",
       " 'the',\n",
       " 'horseshoe',\n",
       " 'the',\n",
       " 'were',\n",
       " 'the',\n",
       " 'centre',\n",
       " 'some',\n",
       " 'sort',\n",
       " 'whirlpool',\n",
       " 'stay',\n",
       " 'may',\n",
       " 'very',\n",
       " 'must',\n",
       " 'ask',\n",
       " 'the',\n",
       " 'Count',\n",
       " 'all',\n",
       " 'about',\n",
       " 'them',\n",
       " 'did',\n",
       " 'not',\n",
       " 'sleep',\n",
       " 'well',\n",
       " 'though',\n",
       " 'bed',\n",
       " 'was',\n",
       " 'enough',\n",
       " 'for',\n",
       " 'had',\n",
       " 'all',\n",
       " 'sorts',\n",
       " 'queer',\n",
       " 'dreams',\n",
       " 'There',\n",
       " 'was',\n",
       " 'dog',\n",
       " 'howling',\n",
       " 'all',\n",
       " 'night',\n",
       " 'under',\n",
       " 'window',\n",
       " 'which',\n",
       " 'may',\n",
       " 'have',\n",
       " 'had',\n",
       " 'something',\n",
       " 'with',\n",
       " 'may',\n",
       " 'have',\n",
       " 'been',\n",
       " 'the',\n",
       " 'paprika',\n",
       " 'for',\n",
       " 'had',\n",
       " 'drink',\n",
       " 'all',\n",
       " 'the',\n",
       " 'water',\n",
       " 'carafe',\n",
       " 'and',\n",
       " 'was',\n",
       " 'still',\n",
       " 'thirsty',\n",
       " 'Towards',\n",
       " 'morning',\n",
       " 'slept',\n",
       " 'and',\n",
       " 'was',\n",
       " 'wakened',\n",
       " 'the',\n",
       " 'continuous',\n",
       " 'knocking',\n",
       " 'door',\n",
       " 'guess',\n",
       " 'must',\n",
       " 'have',\n",
       " 'been',\n",
       " 'sleeping',\n",
       " 'soundly',\n",
       " 'then',\n",
       " 'had',\n",
       " 'for',\n",
       " 'breakfast',\n",
       " 'more',\n",
       " 'paprika',\n",
       " 'and',\n",
       " 'sort',\n",
       " 'porridge',\n",
       " 'maize',\n",
       " 'flour',\n",
       " 'which',\n",
       " 'they',\n",
       " 'said',\n",
       " 'was',\n",
       " 'mamaliga',\n",
       " 'and',\n",
       " 'egg',\n",
       " 'plant',\n",
       " 'stuffed',\n",
       " 'with',\n",
       " 'forcemeat',\n",
       " 'very',\n",
       " 'excellent',\n",
       " 'dish',\n",
       " 'which',\n",
       " 'they',\n",
       " 'call',\n",
       " 'impletata',\n",
       " 'get',\n",
       " 'recipe',\n",
       " 'for',\n",
       " 'this',\n",
       " 'also',\n",
       " 'had',\n",
       " 'hurry',\n",
       " 'breakfast',\n",
       " 'for',\n",
       " 'the',\n",
       " 'train',\n",
       " 'started',\n",
       " 'little',\n",
       " 'before',\n",
       " 'eight',\n",
       " 'rather',\n",
       " 'ought',\n",
       " 'have',\n",
       " 'done',\n",
       " 'for',\n",
       " 'after',\n",
       " 'rushing',\n",
       " 'the',\n",
       " 'station',\n",
       " 'had',\n",
       " 'sit',\n",
       " 'the',\n",
       " 'carriage',\n",
       " 'for',\n",
       " 'more',\n",
       " 'than',\n",
       " 'hour',\n",
       " 'before',\n",
       " 'began',\n",
       " 'move',\n",
       " 'seems',\n",
       " 'that',\n",
       " 'the',\n",
       " 'further',\n",
       " 'east',\n",
       " 'you',\n",
       " 'the',\n",
       " 'more',\n",
       " 'unpunctual',\n",
       " 'are',\n",
       " 'the',\n",
       " 'trains',\n",
       " 'What',\n",
       " 'ought',\n",
       " 'they',\n",
       " 'China',\n",
       " 'All',\n",
       " 'day',\n",
       " 'long',\n",
       " 'seemed',\n",
       " 'dawdle',\n",
       " 'through',\n",
       " 'country',\n",
       " 'which',\n",
       " 'was',\n",
       " 'full',\n",
       " 'beauty',\n",
       " 'every',\n",
       " 'kind',\n",
       " 'Sometimes',\n",
       " 'saw',\n",
       " 'little',\n",
       " 'towns',\n",
       " 'castles',\n",
       " 'the',\n",
       " 'top',\n",
       " 'steep',\n",
       " 'hills',\n",
       " 'such',\n",
       " 'see',\n",
       " 'old',\n",
       " 'missals',\n",
       " 'sometimes',\n",
       " 'ran',\n",
       " 'rivers',\n",
       " 'and',\n",
       " 'streams',\n",
       " 'which',\n",
       " 'seemed',\n",
       " 'from',\n",
       " 'the',\n",
       " 'wide',\n",
       " 'stony',\n",
       " 'margin',\n",
       " 'each',\n",
       " 'side',\n",
       " 'them',\n",
       " 'subject',\n",
       " 'great',\n",
       " 'floods',\n",
       " 'takes',\n",
       " 'lot',\n",
       " 'water',\n",
       " 'and',\n",
       " 'running',\n",
       " 'strong',\n",
       " 'sweep',\n",
       " 'the',\n",
       " 'outside',\n",
       " 'edge',\n",
       " 'river',\n",
       " 'clear',\n",
       " 'every',\n",
       " 'station',\n",
       " 'there',\n",
       " 'were',\n",
       " 'groups',\n",
       " 'people',\n",
       " 'sometimes',\n",
       " 'crowds',\n",
       " 'and',\n",
       " 'all',\n",
       " 'sorts',\n",
       " 'attire',\n",
       " 'Some',\n",
       " 'them',\n",
       " 'were',\n",
       " 'just',\n",
       " 'like',\n",
       " 'the',\n",
       " 'peasants',\n",
       " 'home',\n",
       " 'those',\n",
       " 'saw',\n",
       " 'coming',\n",
       " 'through',\n",
       " 'France',\n",
       " 'and',\n",
       " 'Germany',\n",
       " 'with',\n",
       " 'short',\n",
       " 'jackets',\n",
       " 'and',\n",
       " 'round',\n",
       " 'hats',\n",
       " 'and',\n",
       " 'home',\n",
       " 'made',\n",
       " 'trousers',\n",
       " 'but',\n",
       " 'others',\n",
       " 'were',\n",
       " 'very',\n",
       " 'The',\n",
       " 'women',\n",
       " 'looked',\n",
       " 'pretty',\n",
       " 'except',\n",
       " 'when',\n",
       " 'you',\n",
       " 'got',\n",
       " 'near',\n",
       " 'them',\n",
       " 'but',\n",
       " 'they',\n",
       " 'were',\n",
       " 'very',\n",
       " 'clumsy',\n",
       " 'about',\n",
       " 'the',\n",
       " 'waist',\n",
       " 'They',\n",
       " 'had',\n",
       " 'all',\n",
       " 'full',\n",
       " 'white',\n",
       " 'sleeves',\n",
       " 'some',\n",
       " 'kind',\n",
       " 'other',\n",
       " 'and',\n",
       " 'most',\n",
       " 'them',\n",
       " 'had',\n",
       " 'big',\n",
       " 'belts',\n",
       " 'with',\n",
       " 'lot',\n",
       " 'strips',\n",
       " 'something',\n",
       " 'fluttering',\n",
       " 'from',\n",
       " 'them',\n",
       " 'like',\n",
       " 'the',\n",
       " 'dresses',\n",
       " 'ballet',\n",
       " 'but',\n",
       " 'course',\n",
       " 'there',\n",
       " 'were',\n",
       " 'petticoats',\n",
       " 'under',\n",
       " 'them',\n",
       " 'The',\n",
       " 'strangest',\n",
       " 'figures',\n",
       " 'saw',\n",
       " 'were',\n",
       " 'the',\n",
       " 'Slovaks',\n",
       " 'who',\n",
       " 'were',\n",
       " 'more',\n",
       " 'barbarian',\n",
       " 'than',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'with',\n",
       " 'their',\n",
       " 'big',\n",
       " 'cow',\n",
       " 'boy',\n",
       " 'hats',\n",
       " 'great',\n",
       " 'baggy',\n",
       " 'dirty',\n",
       " 'white',\n",
       " 'trousers',\n",
       " 'white',\n",
       " 'linen',\n",
       " 'shirts',\n",
       " 'and',\n",
       " 'enormous',\n",
       " 'heavy',\n",
       " 'leather',\n",
       " 'belts',\n",
       " 'nearly',\n",
       " 'foot',\n",
       " 'wide',\n",
       " 'all',\n",
       " 'studded',\n",
       " 'over',\n",
       " 'with',\n",
       " 'brass',\n",
       " 'nails',\n",
       " 'They',\n",
       " 'wore',\n",
       " 'high',\n",
       " 'boots',\n",
       " 'with',\n",
       " 'their',\n",
       " 'trousers',\n",
       " 'tucked',\n",
       " 'into',\n",
       " 'them',\n",
       " 'and',\n",
       " 'had',\n",
       " 'long',\n",
       " 'black',\n",
       " 'hair',\n",
       " 'and',\n",
       " 'heavy',\n",
       " 'black',\n",
       " 'moustaches',\n",
       " 'They',\n",
       " 'are',\n",
       " 'very',\n",
       " 'but',\n",
       " 'not',\n",
       " 'look',\n",
       " 'the',\n",
       " 'stage',\n",
       " 'they',\n",
       " 'would',\n",
       " 'set',\n",
       " 'down',\n",
       " 'once',\n",
       " 'some',\n",
       " 'old',\n",
       " 'Oriental',\n",
       " 'band',\n",
       " 'brigands',\n",
       " 'They',\n",
       " 'are',\n",
       " 'however',\n",
       " 'told',\n",
       " 'very',\n",
       " 'harmless',\n",
       " 'and',\n",
       " 'rather',\n",
       " 'wanting',\n",
       " 'natural',\n",
       " 'self',\n",
       " 'assertion',\n",
       " 'was',\n",
       " 'the',\n",
       " 'dark',\n",
       " 'side',\n",
       " 'twilight',\n",
       " 'when',\n",
       " 'got',\n",
       " 'Bistritz',\n",
       " 'which',\n",
       " 'very',\n",
       " 'old',\n",
       " 'place',\n",
       " 'Being',\n",
       " 'the',\n",
       " 'frontier',\n",
       " 'for',\n",
       " 'the',\n",
       " 'Borgo',\n",
       " 'Pass',\n",
       " 'leads',\n",
       " 'from',\n",
       " 'into',\n",
       " 'Bukovina',\n",
       " 'has',\n",
       " 'had',\n",
       " 'very',\n",
       " 'stormy',\n",
       " 'existence',\n",
       " 'and',\n",
       " 'certainly',\n",
       " 'shows',\n",
       " 'marks',\n",
       " 'Fifty',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'series',\n",
       " 'great',\n",
       " 'fires',\n",
       " 'took',\n",
       " 'place',\n",
       " 'which',\n",
       " 'made',\n",
       " 'terrible',\n",
       " 'havoc',\n",
       " 'five',\n",
       " 'separate',\n",
       " 'occasions',\n",
       " 'the',\n",
       " 'very',\n",
       " 'beginning',\n",
       " 'the',\n",
       " 'century',\n",
       " 'underwent',\n",
       " 'siege',\n",
       " 'three',\n",
       " 'weeks',\n",
       " 'and',\n",
       " 'lost',\n",
       " 'people',\n",
       " 'the',\n",
       " 'casualties',\n",
       " 'war',\n",
       " 'proper',\n",
       " 'being',\n",
       " 'assisted',\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = re.findall(r'(\\b[A-Za-z][a-z]{2,9}\\b)', text_string)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 7483),\n",
       " ('and', 5800),\n",
       " ('that', 2434),\n",
       " ('was', 1869),\n",
       " ('for', 1481),\n",
       " ('not', 1400),\n",
       " ('his', 1384),\n",
       " ('with', 1283),\n",
       " ('you', 1270),\n",
       " ('all', 1118)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(words)\n",
    "top_10_frq_words = counter.most_common(10)\n",
    "top_10_frq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 7483 1.0\n",
      "and 5800 1.29\n",
      "that 2434 3.07\n",
      "was 1869 4.0\n",
      "for 1481 5.05\n",
      "not 1400 5.34\n",
      "his 1384 5.41\n",
      "with 1283 5.83\n",
      "you 1270 5.89\n",
      "all 1118 6.69\n"
     ]
    }
   ],
   "source": [
    "most_frequent = top_10_frq_words[0][1]\n",
    "for i, j in top_10_frq_words:\n",
    "    print(i, j, round(most_frequent/j, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity measure\n",
    "In statistics and related fields, a similarity measure or similarity function or similarity metric is a real-valued function that quantifies the similarity between two objects.\n",
    "<br><br>\n",
    "Levenshtein: The Levenshtein distance is a number that tells you how different two strings are. The higher the number, the more different the two strings are.\n",
    "<br><br>\n",
    "Jaccard: Jaccard Similarity defined as an intersection of two documents divided by the union of that two documents that refer to the number of common words over a total number of words.<br>\n",
    "<img src=\"Jaccard-Similarity.png\" height=\"400\">\n",
    "<br><br>\n",
    "Smith Waterman: https://www.youtube.com/watch?v=lu9ScxSejSE&ab_channel=Bioinformatica\n",
    "\n",
    "https://python.gotrained.com/nltk-edit-distance-jaccard-distance/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (4, 5)]\n"
     ]
    }
   ],
   "source": [
    "# Levenshtein\n",
    "from nltk.metrics import distance\n",
    "s1 = \"rain\"\n",
    "s2 = \"shine\"\n",
    "print(distance.edit_distance(s1, s2))\n",
    "print(distance.edit_distance_align(s1, s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jaccard\n",
    "distance.jaccard_distance(set(s1), set(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smith Waterman\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactic analysis\n",
    "Syntactic analysis is a well-developed area of NLP that deals with the syntax of NL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
